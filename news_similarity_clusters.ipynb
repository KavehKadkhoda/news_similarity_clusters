{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3225fbf8-b563-4633-b859-12cea800f206",
      "metadata": {
        "id": "3225fbf8-b563-4633-b859-12cea800f206",
        "outputId": "243990ce-87d2-486a-c59a-c9c92869d395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Environment variables for Trino connection (provided by the platform)\n",
        "TRINO_HOST = os.environ.get(\"TRINO_HOST\")\n",
        "TRINO_USER = os.environ.get(\"TRINO_USER\")\n",
        "TRINO_CATALOG = os.environ.get(\"TRINO_CATALOG\")\n",
        "\n",
        "# Install required libraries\n",
        "%pip install -q trino==0.332.0 trino[sqlalchemy]\n",
        "%pip install -q datasketch\n",
        "\n",
        "# Connect to the Trino query engine using DBAPI\n",
        "from trino.dbapi import connect\n",
        "\n",
        "conn = connect(\n",
        "    host=TRINO_HOST,\n",
        "    user=TRINO_USER,\n",
        "    catalog=TRINO_CATALOG,\n",
        ")\n",
        "cur = conn.cursor()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c244c68-b850-4fe7-b0da-fc6187faa2e0",
      "metadata": {
        "id": "8c244c68-b850-4fe7-b0da-fc6187faa2e0",
        "outputId": "fee9fccd-a212-4b10-b80a-864aac688e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using date filter: | start_date=2025-09-01 | end_date=2025-09-07\n",
            "Remove duplicate URLs: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from datetime import date\n",
        "\n",
        "# ðŸš¦ Set these to control the time window (strings \"YYYY-MM-DD\") or None for open-ended.\n",
        "# Examples:\n",
        "# start_date = \"2025-09-01\"; end_date = None         -> from 2025-09-01 to end\n",
        "# start_date = \"2025-09-01\"; end_date = \"2025-09-15\" -> within inclusive window\n",
        "# start_date = None; end_date = None                 -> all data\n",
        "start_date = \"2025-09-01\"\n",
        "end_date   = \"2025-09-07\"\n",
        "\n",
        "# ðŸš¦ New variable to control duplicate URL removal (default is to remove duplicates)\n",
        "remove_duplicate_urls = True\n",
        "\n",
        "def _normalize_date_str(d):\n",
        "    \"\"\"\n",
        "    Accepts a value that should be either None or a 'YYYY-MM-DD' string.\n",
        "    - Returns None if input is None or empty after stripping.\n",
        "    - Returns ISO 'YYYY-MM-DD' string if valid.\n",
        "    - Raises ValueError if invalid format.\n",
        "    \"\"\"\n",
        "    if d is None:\n",
        "        return None\n",
        "    s = str(d).strip().strip('\"').strip(\"'\")\n",
        "    if not s:\n",
        "        return None\n",
        "    try:\n",
        "        # Validate format and return normalized ISO string\n",
        "        return date.fromisoformat(s).isoformat()\n",
        "    except Exception:\n",
        "        raise ValueError(f\"Invalid date: {d!r}. Expected format 'YYYY-MM-DD' (e.g., '2025-09-01').\")\n",
        "\n",
        "START_DATE = _normalize_date_str(start_date)\n",
        "END_DATE   = _normalize_date_str(end_date)\n",
        "print(\"Using date filter:\",\n",
        "      f\"start_date={START_DATE}\" if START_DATE else \"start_date=None\",\n",
        "      f\"end_date={END_DATE}\" if END_DATE else \"end_date=None\", sep=\" | \")\n",
        "print(\"Remove duplicate URLs:\", remove_duplicate_urls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19fb7552-9ec9-47b3-9566-fbf81e9aa1c7",
      "metadata": {
        "id": "19fb7552-9ec9-47b3-9566-fbf81e9aa1c7",
        "outputId": "ab7da0e8-ea9b-4407-d48a-6d992a1d26d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 16019 rows from news.collected_news.\n",
            "Sample row: ['https://www.express.co.uk/life-style/garden/2103924/families-urged-take-immediate-action-blackbirds-visit-gardens', 'Families are being asked to take immediate action in their gardens as the deadly mosquito-borne Usutu virus threatens to wipe out blackbird populations across the country. Ecologists are pleading with families to make small changes that could significantly help protect one of our most beloved garden birds.\\n\\nFigures first revealed in June showed a 40% decline in blackbirds in Greater London since the virus was first detected in 2020 - and with climate change helping mosquitoes to expand their territory, experts warn that the problem is only set to worsen. The Usutu virus, which is spread by the Culex pipiens mosquito - also known as the house mosquito - does not harm humans in the UK, but often proves lethal to birds - especially blackbirds, who are particularly prone to infection.\\n\\nWith longer summers, rising temperatures and heavier rainfall providing ideal breeding conditions for mosquitoes, ecologists caution that climate change is allowing diseases like Usutu to establish a foothold in the UK for the first time. The virus is now deeply rooted in Britain\\'s wild bird population and further declines in some species are expected unless steps are taken to alleviate additional pressures such as habitat loss and food scarcity.\\n\\nEcological experts at Arbtech have called on families to support one of Britain\\'s most common garden visitors, whose numbers are under threat.\\n\\nA spokesperson for Arbotech said: \"Blackbirds are more than just a familiar visitor to our gardens - they\\'re important seed dispersers and insect predators, helping to regulate pest populations and support healthy ecosystems.\\n\\n\"But they\\'re under real pressure. Diseases like Usutu virus, which can cause tremors, disorientation and even sudden death in infected birds, are an emerging threat - and many people still don\\'t realise the impact it\\'s already having.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Define the schema and table to fetch data from\n",
        "schema_name = \"news\"\n",
        "table_name  = \"collected_news\"\n",
        "\n",
        "# Build SQL query safely based on optional START_DATE / END_DATE.\n",
        "# created_at is ISO8601, e.g., \"2025-01-21T02:26:32Z\".\n",
        "# We filter by the DATE portion via substr(created_at, 1, 10) = 'YYYY-MM-DD'.\n",
        "if START_DATE and END_DATE:\n",
        "    query = (\n",
        "        f\"SELECT url, text FROM {schema_name}.{table_name} \"\n",
        "        f\"WHERE substr(created_at, 1, 10) >= '{START_DATE}' \"\n",
        "        f\"  AND substr(created_at, 1, 10) <= '{END_DATE}'\"\n",
        "    )\n",
        "elif START_DATE:\n",
        "    query = (\n",
        "        f\"SELECT url, text FROM {schema_name}.{table_name} \"\n",
        "        f\"WHERE substr(created_at, 1, 10) >= '{START_DATE}'\"\n",
        "    )\n",
        "elif END_DATE:\n",
        "    query = (\n",
        "        f\"SELECT url, text FROM {schema_name}.{table_name} \"\n",
        "        f\"WHERE substr(created_at, 1, 10) <= '{END_DATE}'\"\n",
        "    )\n",
        "else:\n",
        "    query = f\"SELECT url, text FROM {schema_name}.{table_name}\"\n",
        "\n",
        "# Execute the query and fetch results\n",
        "cur.execute(query)\n",
        "rows = cur.fetchall()\n",
        "\n",
        "print(f\"Retrieved {len(rows)} rows from {schema_name}.{table_name}.\")\n",
        "if rows:\n",
        "    print(\"Sample row:\", rows[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63650dbd-abc6-4a86-9d3e-120ae227bff8",
      "metadata": {
        "id": "63650dbd-abc6-4a86-9d3e-120ae227bff8",
        "outputId": "dad61930-d076-49af-b66d-bc5cfbe08cc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed 5184 duplicate URLs (from 16019 records).\n",
            "Number of articles loaded into dictionary: 10835\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Optionally remove duplicate URLs before analysis\n",
        "if remove_duplicate_urls:\n",
        "    df = pd.DataFrame(rows, columns=[\"url\", \"text\"])\n",
        "    before_count = len(df)\n",
        "    df = df.drop_duplicates(subset=\"url\", keep=\"first\")\n",
        "    after_count = len(df)\n",
        "    removed = before_count - after_count\n",
        "    print(f\"Removed {removed} duplicate URLs (from {before_count} records).\")\n",
        "    # Update rows to the deduplicated list of (url, text) pairs\n",
        "    rows = df[[\"url\", \"text\"]].values.tolist()\n",
        "\n",
        "# Arrange data in a dictionary: {id: text}, where id is the URL\n",
        "articles_dict = {url: text for url, text in rows}\n",
        "print(f\"Number of articles loaded into dictionary: {len(articles_dict)}\")\n",
        "\n",
        "# Prepare a list of (id, text) tuples for analysis\n",
        "articles_list = list(articles_dict.items())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2de1501-b284-4a83-897b-d202e78cc066",
      "metadata": {
        "id": "f2de1501-b284-4a83-897b-d202e78cc066"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "\n",
        "class NearDuplicateDetector:\n",
        "    \"\"\"\n",
        "    Language-independent near-duplicate detection using character shingles + MinHash + LSH.\n",
        "    \"\"\"\n",
        "    def __init__(self, threshold: float = 0.8, num_perm: int = 128, shingle_size: int = 5):\n",
        "        self.threshold = float(threshold)\n",
        "        self.num_perm = int(num_perm)\n",
        "        self.shingle_size = int(shingle_size)\n",
        "        self._lsh = MinHashLSH(threshold=self.threshold, num_perm=self.num_perm)\n",
        "        self._minhashes = {}  # url -> MinHash signature\n",
        "\n",
        "    def _preprocess(self, text: str) -> str:\n",
        "        # Normalize whitespace and lowercase for robustness\n",
        "        cleaned = ' '.join((text or \"\").split())\n",
        "        return cleaned.lower()\n",
        "\n",
        "    def _get_shingles(self, text: str) -> set:\n",
        "        # Character shingles of length shingle_size\n",
        "        cleaned_text = self._preprocess(text)\n",
        "        n = self.shingle_size\n",
        "        if len(cleaned_text) < n:\n",
        "            return {cleaned_text}\n",
        "        return {cleaned_text[i:i+n] for i in range(len(cleaned_text) - n + 1)}\n",
        "\n",
        "    def _minhash_signature(self, shingles: set) -> MinHash:\n",
        "        m = MinHash(num_perm=self.num_perm)\n",
        "        for sh in shingles:\n",
        "            m.update(sh.encode(\"utf-8\"))\n",
        "        return m\n",
        "\n",
        "    def add_article(self, url: str, text: str) -> list:\n",
        "        \"\"\"\n",
        "        Adds one article and returns list of prior URLs similar to it (>= threshold).\n",
        "        \"\"\"\n",
        "        shingles = self._get_shingles(text)\n",
        "        m = self._minhash_signature(shingles)\n",
        "        similar_urls = []\n",
        "        if self._minhashes:\n",
        "            candidates = self._lsh.query(m)  # approximate neighborhood\n",
        "            for cand_url in candidates:\n",
        "                # Compute exact Jaccard similarity for candidates\n",
        "                sim = m.jaccard(self._minhashes[cand_url])\n",
        "                if sim >= self.threshold:\n",
        "                    similar_urls.append(cand_url)\n",
        "        # Insert after querying to avoid self-matching\n",
        "        self._lsh.insert(url, m)\n",
        "        self._minhashes[url] = m\n",
        "        return similar_urls\n",
        "\n",
        "    def add_articles(self, articles: list) -> dict:\n",
        "        \"\"\"\n",
        "        Batch add [(url, text), ...]. Returns {url: [similar_urls]} for each URL that had matches.\n",
        "        \"\"\"\n",
        "        duplicates_found = {}\n",
        "        for url, text in articles:\n",
        "            similar = self.add_article(url, text)\n",
        "            if similar:\n",
        "                duplicates_found[url] = similar\n",
        "        return duplicates_found\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a094da-9eec-4fee-a822-c92f185fde7a",
      "metadata": {
        "id": "b6a094da-9eec-4fee-a822-c92f185fde7a",
        "outputId": "1033ceab-f744-470d-9198-3f4b880132cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detector initialized: threshold=0.9, num_perm=128, shingle_size=5\n",
            "Articles with at least one near-duplicate: 3903\n",
            "- https://www.bournemouthecho.co.uk/news/national/25446659.farage-ends-conference-unity-plea-rowing-back-small-boats-pledge/ ~ ['https://www.edp24.co.uk/news/national/25446659.farage-ends-conference-unity-plea-rowing-back-small-boats-pledge/']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Configure similarity threshold & shingling\n",
        "THRESHOLD = 0.9   # adjust as needed (e.g., 0.85 or 0.9 for stricter matching)\n",
        "NUM_PERM  = 128\n",
        "SHINGLE_N = 5\n",
        "\n",
        "detector = NearDuplicateDetector(threshold=THRESHOLD, num_perm=NUM_PERM, shingle_size=SHINGLE_N)\n",
        "print(f\"Detector initialized: threshold={THRESHOLD}, num_perm={NUM_PERM}, shingle_size={SHINGLE_N}\")\n",
        "\n",
        "# Batch process articles to find near-duplicates\n",
        "duplicates_found = detector.add_articles(articles_list)\n",
        "\n",
        "print(f\"Articles with at least one near-duplicate: {len(duplicates_found)}\")\n",
        "# Optional peek at one example\n",
        "for art_id, dup_ids in list(duplicates_found.items())[:5]:\n",
        "    print(f\"- {art_id} ~ {dup_ids}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7911551d-1706-44b2-b812-b5d5739ab161",
      "metadata": {
        "id": "7911551d-1706-44b2-b812-b5d5739ab161",
        "outputId": "eb594d42-929d-4aae-bd30-8dcc4c5444a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique near-duplicate pairs: 64017\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_1</th>\n",
              "      <th>id_2</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>http://maremmanews.it/post/allarme-oms-un-adol...</td>\n",
              "      <td>https://www.imgpress.it/culture/un-adolescente...</td>\n",
              "      <td>0.984375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>http://stranieriinitalia.it/attualita/il-papa-...</td>\n",
              "      <td>https://stranieriinitalia.it/attualita/il-papa...</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>http://stranieriinitalia.it/attualita/nave-ong...</td>\n",
              "      <td>https://stranieriinitalia.it/attualita/nave-on...</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>http://stranieriinitalia.it/attualita/prato-co...</td>\n",
              "      <td>https://stranieriinitalia.it/attualita/prato-c...</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>http://www.az-online.de/boulevard/der-skandals...</td>\n",
              "      <td>http://www.hna.de/leute/der-skandalstar-ist-la...</td>\n",
              "      <td>0.992188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>http://www.az-online.de/boulevard/der-skandals...</td>\n",
              "      <td>http://www.merkur.de/boulevard/der-skandalstar...</td>\n",
              "      <td>0.992188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>http://www.az-online.de/boulevard/der-skandals...</td>\n",
              "      <td>http://www.tz.de/stars/der-skandalstar-ist-lae...</td>\n",
              "      <td>0.992188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>http://www.az-online.de/boulevard/der-skandals...</td>\n",
              "      <td>https://www.tageblatt.de/Nachrichten/Der-Skand...</td>\n",
              "      <td>0.945312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>http://www.az-online.de/deutschland/spd-droht-...</td>\n",
              "      <td>http://www.hna.de/welt/spd-droht-wegen-afd-kom...</td>\n",
              "      <td>0.992188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>http://www.az-online.de/deutschland/spd-droht-...</td>\n",
              "      <td>http://www.kreisbote.de/welt/spd-droht-wegen-a...</td>\n",
              "      <td>0.992188</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                id_1  \\\n",
              "0  http://maremmanews.it/post/allarme-oms-un-adol...   \n",
              "1  http://stranieriinitalia.it/attualita/il-papa-...   \n",
              "2  http://stranieriinitalia.it/attualita/nave-ong...   \n",
              "3  http://stranieriinitalia.it/attualita/prato-co...   \n",
              "4  http://www.az-online.de/boulevard/der-skandals...   \n",
              "5  http://www.az-online.de/boulevard/der-skandals...   \n",
              "6  http://www.az-online.de/boulevard/der-skandals...   \n",
              "7  http://www.az-online.de/boulevard/der-skandals...   \n",
              "8  http://www.az-online.de/deutschland/spd-droht-...   \n",
              "9  http://www.az-online.de/deutschland/spd-droht-...   \n",
              "\n",
              "                                                id_2  similarity  \n",
              "0  https://www.imgpress.it/culture/un-adolescente...    0.984375  \n",
              "1  https://stranieriinitalia.it/attualita/il-papa...    1.000000  \n",
              "2  https://stranieriinitalia.it/attualita/nave-on...    1.000000  \n",
              "3  https://stranieriinitalia.it/attualita/prato-c...    1.000000  \n",
              "4  http://www.hna.de/leute/der-skandalstar-ist-la...    0.992188  \n",
              "5  http://www.merkur.de/boulevard/der-skandalstar...    0.992188  \n",
              "6  http://www.tz.de/stars/der-skandalstar-ist-lae...    0.992188  \n",
              "7  https://www.tageblatt.de/Nachrichten/Der-Skand...    0.945312  \n",
              "8  http://www.hna.de/welt/spd-droht-wegen-afd-kom...    0.992188  \n",
              "9  http://www.kreisbote.de/welt/spd-droht-wegen-a...    0.992188  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd  # (Pandas already imported above, but included here for clarity)\n",
        "\n",
        "# Build list of undirected unique pairs and compute similarities\n",
        "pairs = []\n",
        "pairs_with_sim = []\n",
        "for new_id, similar_ids in duplicates_found.items():\n",
        "    for existing_id in similar_ids:\n",
        "        if new_id == existing_id:\n",
        "            # Skip self-duplicates (should not happen if duplicates were removed)\n",
        "            continue\n",
        "        # Sort the pair for uniqueness (undirected pair)\n",
        "        pair_sorted = tuple(sorted((existing_id, new_id)))\n",
        "        pairs.append(pair_sorted)\n",
        "        sim = detector._minhashes[new_id].jaccard(detector._minhashes[existing_id])\n",
        "        pairs_with_sim.append((pair_sorted[0], pair_sorted[1], sim))\n",
        "\n",
        "# Deduplicate pairs (ensure each pair is only listed once)\n",
        "pairs = sorted(set(pairs))\n",
        "\n",
        "# Build DataFrame of duplicate pairs with similarity\n",
        "duplicates_df = pd.DataFrame(pairs_with_sim, columns=[\"id_1\", \"id_2\", \"similarity\"])\n",
        "# If any duplicate rows exist (unlikely after set), drop them and keep the highest similarity\n",
        "duplicates_df = (duplicates_df\n",
        "                 .sort_values([\"id_1\", \"id_2\", \"similarity\"], ascending=[True, True, False])\n",
        "                 .drop_duplicates(subset=[\"id_1\", \"id_2\"])\n",
        "                 .reset_index(drop=True)\n",
        "                )\n",
        "\n",
        "print(f\"Total unique near-duplicate pairs: {len(duplicates_df)}\")\n",
        "display(duplicates_df.head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4adb1316-db34-4e73-89c6-c52c0e6cf161",
      "metadata": {
        "id": "4adb1316-db34-4e73-89c6-c52c0e6cf161",
        "outputId": "a79d0df7-cee2-4133-d27f-67277cc20505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output saved to 'duplicates_pairs.json' and 'duplicates_mapping.json'.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import json\n",
        "\n",
        "# Save the duplicate pairs DataFrame to a JSON file (list of records)\n",
        "duplicates_df.to_json(\"duplicates_pairs.json\", orient=\"records\", lines=False)\n",
        "\n",
        "# Save the duplicates mapping (each article -> similar articles) to a JSON file\n",
        "with open(\"duplicates_mapping.json\", \"w\") as f:\n",
        "    json.dump(duplicates_found, f)\n",
        "\n",
        "print(\"Output saved to 'duplicates_pairs.json' and 'duplicates_mapping.json'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "897d1d8f-9e85-492a-bdac-282f14a25e1e",
      "metadata": {
        "id": "897d1d8f-9e85-492a-bdac-282f14a25e1e",
        "outputId": "4df2765f-0504-4789-eb8c-f87c2a3ae975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total clusters of near-duplicates: 806\n",
            "Output saved to 'duplicates_clusters.json'.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Build adjacency list for near-duplicate pairs (undirected graph)\n",
        "adjacency = {}\n",
        "for id1, id2 in pairs:\n",
        "    adjacency.setdefault(id1, set()).add(id2)\n",
        "    adjacency.setdefault(id2, set()).add(id1)\n",
        "\n",
        "# Traverse the graph to get connected components (clusters of similar articles)\n",
        "clusters = []\n",
        "visited = set()\n",
        "for node in adjacency:\n",
        "    if node not in visited:\n",
        "        cluster_set = set()\n",
        "        stack = [node]\n",
        "        visited.add(node)\n",
        "        while stack:\n",
        "            n = stack.pop()\n",
        "            cluster_set.add(n)\n",
        "            for neigh in adjacency[n]:\n",
        "                if neigh not in visited:\n",
        "                    visited.add(neigh)\n",
        "                    stack.append(neigh)\n",
        "        clusters.append(cluster_set)\n",
        "\n",
        "# Filter out clusters of size 1 (articles with no near-duplicates)\n",
        "clusters = [c for c in clusters if len(c) > 1]\n",
        "\n",
        "# Convert each set to a sorted list for JSON serialization\n",
        "clusters_list = [sorted(list(c)) for c in clusters]\n",
        "\n",
        "# Save the clusters of similar articles to a JSON file\n",
        "with open(\"duplicates_clusters.json\", \"w\") as f:\n",
        "    json.dump(clusters_list, f)\n",
        "\n",
        "print(f\"Total clusters of near-duplicates: {len(clusters_list)}\")\n",
        "print(\"Output saved to 'duplicates_clusters.json'.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}